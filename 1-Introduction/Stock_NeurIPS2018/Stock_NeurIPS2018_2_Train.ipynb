{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMjwq6pS-kFz"
   },
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT-zXutMgqOS"
   },
   "source": [
    "# Part 1. Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "D0vEcPxSJ8hI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (4.1.1)\n",
      "Requirement already satisfied: wrds in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (3.1.6)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds) (1.26.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds) (2.1.1)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds) (1.11.3)\n",
      "Requirement already satisfied: sqlalchemy<2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds) (1.4.49)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas->wrds) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas->wrds) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas->wrds) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->wrds) (1.16.0)\n",
      "Requirement already satisfied: pyportfolioopt in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (1.5.5)\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyportfolioopt) (1.4.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyportfolioopt) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyportfolioopt) (2.1.1)\n",
      "Requirement already satisfied: scipy<2.0,>=1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyportfolioopt) (1.11.3)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.3)\n",
      "Requirement already satisfied: ecos>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.12)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.0)\n",
      "Requirement already satisfied: scs>=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.3)\n",
      "Requirement already satisfied: pybind11 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2023.3)\n",
      "Requirement already satisfied: qdldl in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
      "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
      "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /private/var/folders/pc/8l9dz1f949ddzztd4yfcytx80000gn/T/pip-req-build-hnbhwp3e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /private/var/folders/pc/8l9dz1f949ddzztd4yfcytx80000gn/T/pip-req-build-hnbhwp3e\n",
      "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit 7c71056dd6d72e205096696319a2d8bd4a2bfe23\n",
      "  Installing build dependencies ... \u001B[?25ldone\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n",
      "\u001B[?25hCollecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
      "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /private/var/folders/pc/8l9dz1f949ddzztd4yfcytx80000gn/T/pip-install-yvrctbtl/elegantrl_f6e392a5a7f4448ca1a4f166b992b1bd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /private/var/folders/pc/8l9dz1f949ddzztd4yfcytx80000gn/T/pip-install-yvrctbtl/elegantrl_f6e392a5a7f4448ca1a4f166b992b1bd\n",
      "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit a68515548417093006eb7f68738b55a3a758645e\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\n",
      "\u001B[?25hRequirement already satisfied: alpaca-trade-api<4,>=3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (3.0.2)\n",
      "Requirement already satisfied: ccxt<4,>=3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (3.1.60)\n",
      "Requirement already satisfied: exchange-calendars<5,>=4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (4.5)\n",
      "Requirement already satisfied: jqdatasdk<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (1.9.1)\n",
      "Requirement already satisfied: pyfolio<0.10,>=0.9 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (0.9.2)\n",
      "Requirement already satisfied: pyportfolioopt<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (1.5.5)\n",
      "Requirement already satisfied: ray[default,tune]<3,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: stockstats<0.6,>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (0.5.4)\n",
      "Requirement already satisfied: wrds<4,>=3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (3.1.6)\n",
      "Requirement already satisfied: yfinance<0.3,>=0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from finrl==0.3.6) (0.2.31)\n",
      "Requirement already satisfied: pandas>=0.18.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.0)\n",
      "Requirement already satisfied: requests<3,>2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
      "Requirement already satisfied: urllib3<2,>1.24 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.26.17)\n",
      "Requirement already satisfied: websocket-client<2,>=0.56.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.6.4)\n",
      "Requirement already satisfied: websockets<11,>=9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (10.4)\n",
      "Requirement already satisfied: msgpack==1.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.0.3)\n",
      "Requirement already satisfied: aiohttp==3.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.8.2)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0)\n",
      "Requirement already satisfied: deprecation==2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.1.1)\n",
      "Requirement already satisfied: multidict<6.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiohttp==3.8.2->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2)\n",
      "Requirement already satisfied: setuptools>=60.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (68.0.0)\n",
      "Requirement already satisfied: certifi>=2018.1.18 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (2023.7.22)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (41.0.4)\n",
      "Requirement already satisfied: aiodns>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ccxt<4,>=3->finrl==0.3.6) (3.1.0)\n",
      "Requirement already satisfied: pyluach in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2.8.2)\n",
      "Requirement already satisfied: toolz in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.0)\n",
      "Requirement already satisfied: tzdata in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2023.3)\n",
      "Requirement already satisfied: korean-lunar-calendar in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.3.1)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.4.49)\n",
      "Requirement already satisfied: thriftpy2>=0.3.9 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (0.4.17)\n",
      "Requirement already satisfied: pymysql>=0.7.6 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.1.0)\n",
      "Requirement already satisfied: ipython>=3.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (8.16.1)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.8.0)\n",
      "Requirement already satisfied: pytz>=2014.10 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.3.post1)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.11.3)\n",
      "Requirement already satisfied: seaborn>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.0)\n",
      "Requirement already satisfied: empyrical>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.4.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.12.4)\n",
      "Requirement already satisfied: jsonschema in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.19.1)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.24.4)\n",
      "Requirement already satisfied: aiohttp-cors in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: colorful in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.5.5)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.14)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.1.1)\n",
      "Requirement already satisfied: opencensus in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.11.3)\n",
      "Requirement already satisfied: pydantic<2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.10.13)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.17.1)\n",
      "Requirement already satisfied: smart-open in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.4.0)\n",
      "Requirement already satisfied: virtualenv<20.21.1,>=20.0.24 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.21.0)\n",
      "Requirement already satisfied: grpcio>=1.42.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.59.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.6.2.2)\n",
      "Requirement already satisfied: pyarrow>=6.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (13.0.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2023.9.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.2.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.29.1)\n",
      "Requirement already satisfied: torch>=1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: cloudpickle in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.1)\n",
      "Requirement already satisfied: opencv-python in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.1.78)\n",
      "Requirement already satisfied: pygame in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.0)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.14.1)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.1)\n",
      "Requirement already satisfied: rich in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.6.0)\n",
      "Requirement already satisfied: shimmy[atari]~=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.1.0)\n",
      "Requirement already satisfied: pillow in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (10.0.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from wrds<4,>=3->finrl==0.3.6) (2.9.9)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.3)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.4.4)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.3.8)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.12.2)\n",
      "Requirement already satisfied: html5lib>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
      "Requirement already satisfied: gym in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.26.2)\n",
      "Requirement already satisfied: pycares>=4.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6) (4.4.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.6.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.16.0)\n",
      "Requirement already satisfied: osqp>=0.6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.3)\n",
      "Requirement already satisfied: ecos>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.12)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.0)\n",
      "Requirement already satisfied: scs>=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.3)\n",
      "Requirement already satisfied: pybind11 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.11.1)\n",
      "Requirement already satisfied: pandas-datareader>=0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (12.535.108)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.0.4)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
      "Requirement already satisfied: backcall in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.11.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.8.0)\n",
      "Requirement already satisfied: appnope in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.23.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: ply<4.0,>=3.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6) (3.11)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.2)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (0.3.7)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from virtualenv<20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6) (3.11.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.0.8)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (2.3.5)\n",
      "Requirement already satisfied: swig==4.* in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from gym->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (4.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jsonschema->ray[default,tune]<3,>=2->finrl==0.3.6) (0.10.6)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.12.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (6.1.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default,tune]<3,>=2->finrl==0.3.6) (0.2.8)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (2.21)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.61.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
      "Requirement already satisfied: qdldl in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.1.7.post0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from stack-data->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/finRL/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "## install required packages\n",
    "!pip install swig\n",
    "!pip install wrds\n",
    "!pip install pyportfolioopt\n",
    "## install finrl library\n",
    "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xt1317y2ixSS",
    "ExecuteTime": {
     "end_time": "2024-08-27T04:10:12.299094Z",
     "start_time": "2024-08-27T04:10:07.119974Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWrSrQv3i0Ng"
   },
   "source": [
    "# Part 2. Build A Market Environment in OpenAI Gym-style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiHhM2U-XBMZ"
   },
   "source": [
    "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeneTRdyZDvy"
   },
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3H88JXkI93v"
   },
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKyZejI0fmp1"
   },
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mFCP1YEhi6oi",
    "ExecuteTime": {
     "end_time": "2024-08-27T04:12:52.108075Z",
     "start_time": "2024-08-27T04:12:51.877612Z"
    }
   },
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yw95ZMicgEyi"
   },
   "source": [
    "## Construct the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WZ6-9q2gq9S"
   },
   "source": [
    "Calculate and specify the parameters we need for constructing the environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7T3DZPoaIm8k",
    "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:06:00.000346Z",
     "start_time": "2024-08-27T05:05:59.978316Z"
    }
   },
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension # 当前的cash，每个股票的close/仓位/技术指标\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WsOLoeNcJF8Q",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:09:05.419688Z",
     "start_time": "2024-08-27T05:09:05.394747Z"
    }
   },
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension # 买入和卖出的手续费\n",
    "num_stock_shares = [0] * stock_dimension # 每个股票的持有量\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, # 最大持有量\n",
    "    \"initial_amount\": 1000000, # 初始资金\n",
    "    \"num_stock_shares\": num_stock_shares, # 每个股票的持有量\n",
    "    \"buy_cost_pct\": buy_cost_list, # 买入手续费\n",
    "    \"sell_cost_pct\": sell_cost_list, # 卖出手续费\n",
    "    \"state_space\": state_space, # 状态空间\n",
    "    \"stock_dim\": stock_dimension, # 股票维度\n",
    "    \"tech_indicator_list\": INDICATORS, # 技术指标\n",
    "    \"action_space\": stock_dimension, # 动作空间\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7We-q73jjaFQ"
   },
   "source": [
    "## Environment for training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aS-SHiGRJK-4",
    "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:09:35.062579Z",
     "start_time": "2024-08-27T05:09:35.047545Z"
    }
   },
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "364PsqckttcQ",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:10:19.409960Z",
     "start_time": "2024-08-27T05:10:19.398960Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDmqOyF9h1iz"
   },
   "source": [
    "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uijiWgkuh1jB"
   },
   "source": [
    "### Agent 1: A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GUCnkn-HIbmj",
    "outputId": "2794a094-a916-448c-ead1-6e20184dde2a",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:11:54.056534Z",
     "start_time": "2024-08-27T05:11:52.605367Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GVpkWGqH4-D",
    "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:22:25.081724Z",
     "start_time": "2024-08-27T05:11:54.138925Z"
    }
   },
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 31          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.994      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -35.2       |\n",
      "|    reward             | -0.34033167 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.32        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 45        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0754    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -40.1     |\n",
      "|    reward             | -1.486165 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.99      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 53       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -309     |\n",
      "|    reward             | 5.58815  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 54.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 58        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 55.2      |\n",
      "|    reward             | 3.5159886 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.26      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 61         |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 442        |\n",
      "|    reward             | -10.674451 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 163        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 164        |\n",
      "|    reward             | 0.10936774 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 20.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 65         |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0132     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 63.7       |\n",
      "|    reward             | -5.6901636 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.84       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 121       |\n",
      "|    reward             | -1.442073 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 14.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 184       |\n",
      "|    reward             | 6.5165224 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 31.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 76.5       |\n",
      "|    reward             | -10.852199 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.05e+03 |\n",
      "|    reward             | 21.054466 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 733       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 70           |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 84           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.2        |\n",
      "|    explained_variance | 0.00935      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -152         |\n",
      "|    reward             | -0.006515713 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 17.1         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 57.4       |\n",
      "|    reward             | -5.1679044 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 18.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 71       |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0.0197   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 231      |\n",
      "|    reward             | 1.560801 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 62.4     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 72       |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 140      |\n",
      "|    reward             | 5.012915 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 19       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.234      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 163        |\n",
      "|    reward             | 0.19053265 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 73        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -374      |\n",
      "|    reward             | 6.7864656 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 117       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -15.6    |\n",
      "|    reward             | 2.393841 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.2      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 73         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 32         |\n",
      "|    reward             | 0.88161665 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.74       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -43.6     |\n",
      "|    reward             | -3.248012 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 83.6      |\n",
      "|    reward             | 5.8145547 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -55        |\n",
      "|    reward             | -14.019494 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -3.25e+03 |\n",
      "|    reward             | 0.4511991 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.01e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 76        |\n",
      "|    reward             | 0.4870684 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 165         |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -67.4       |\n",
      "|    reward             | -0.56255764 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.74        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -21.1    |\n",
      "|    reward             | 2.800022 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 0.588    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 75          |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 178         |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | -34.9       |\n",
      "|    reward             | -0.77146965 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 1.56        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.066     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 683       |\n",
      "|    reward             | 0.7692335 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 314       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -149      |\n",
      "|    reward             | 3.8758578 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 14.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 196        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 86.3       |\n",
      "|    reward             | 0.34555504 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.1        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 202         |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 130         |\n",
      "|    reward             | -0.30751148 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 10          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 208        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -44.4      |\n",
      "|    reward             | -2.9590316 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 214       |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.125     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -6.15     |\n",
      "|    reward             | 0.6885842 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.629     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.00204  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 176       |\n",
      "|    reward             | 9.2435255 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 226        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0169     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 115        |\n",
      "|    reward             | 0.41995996 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 233        |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.00369    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | 0.42753348 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 288      |\n",
      "|    reward             | 1.223142 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 49.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 245        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 72.7       |\n",
      "|    reward             | 0.27306947 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 251       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -287      |\n",
      "|    reward             | 3.6083188 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 48.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 257      |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -83.1    |\n",
      "|    reward             | 4.648542 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 19.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 264        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -13.6      |\n",
      "|    reward             | 0.81684405 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 270         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | -129        |\n",
      "|    reward             | -0.18370555 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 13.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 277       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -106      |\n",
      "|    reward             | 3.7756517 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.94      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 284      |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 193      |\n",
      "|    reward             | 3.870911 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 25.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 290         |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | 223         |\n",
      "|    reward             | -0.53409857 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 28.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 297       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | 4.322756  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 19.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 303        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.0016     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -166       |\n",
      "|    reward             | 0.20345427 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 24.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 310         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -238        |\n",
      "|    reward             | -0.29786557 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 37.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 316       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -46.6     |\n",
      "|    reward             | 1.7582315 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.54      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 322        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | -63.4      |\n",
      "|    reward             | 0.28916633 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.48       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 328       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 514       |\n",
      "|    reward             | 0.9540378 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 193       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 335       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -195      |\n",
      "|    reward             | 14.669389 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 96.1      |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7990552.23\n",
      "total_reward: 6990552.23\n",
      "total_cost: 5912.73\n",
      "total_trades: 44343\n",
      "Sharpe: 1.061\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 341       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 1.0110708 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.141     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 77          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 347         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | -0.00207    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -154        |\n",
      "|    reward             | -0.47156507 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 16          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 353       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.000727 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -40.6     |\n",
      "|    reward             | 4.1178927 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 359       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -20.1     |\n",
      "|    reward             | -0.482903 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 365       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -381      |\n",
      "|    reward             | -6.221893 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 121       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 371       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -0.0634   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 1.58      |\n",
      "|    reward             | 2.1810446 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.74      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 377         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 5.06        |\n",
      "|    reward             | -0.37894848 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.294       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 384         |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | -4.34       |\n",
      "|    reward             | -0.58371717 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.22        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 390        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -29.1      |\n",
      "|    reward             | -4.9877005 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.39       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 397         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 15.1        |\n",
      "|    reward             | -0.33027428 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.82        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 403       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 674       |\n",
      "|    reward             | 15.201488 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 367       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 409       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 31.9      |\n",
      "|    reward             | 2.5331366 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.919     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 416        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 5.92       |\n",
      "|    reward             | -4.5249476 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.66       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 422         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -117        |\n",
      "|    reward             | -0.41255113 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 14.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 428        |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -965       |\n",
      "|    reward             | -10.767748 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 799        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 434       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -316      |\n",
      "|    reward             | 1.1309323 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 72.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 440        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -410       |\n",
      "|    reward             | -3.6296144 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 141        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 446        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 47         |\n",
      "|    reward             | -0.3754183 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 452       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 19.8      |\n",
      "|    reward             | 1.8316387 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.575     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 459        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -306       |\n",
      "|    reward             | 0.27641746 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 59.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 465       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -96       |\n",
      "|    reward             | 2.0333526 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 20.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 471       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 202       |\n",
      "|    reward             | -14.62616 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 51.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 477        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 535        |\n",
      "|    reward             | -19.199753 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 203        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 483       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -127      |\n",
      "|    reward             | 1.5217638 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 490       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -69.5     |\n",
      "|    reward             | 1.4004798 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 496        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -80.8      |\n",
      "|    reward             | 0.61736494 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 502       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 378       |\n",
      "|    reward             | 4.2622886 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 110       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 508        |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -242       |\n",
      "|    reward             | -3.7418916 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 44.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 514       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 332       |\n",
      "|    reward             | 12.031523 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 99.4      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 78           |\n",
      "|    iterations         | 8200         |\n",
      "|    time_elapsed       | 520          |\n",
      "|    total_timesteps    | 41000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8199         |\n",
      "|    policy_loss        | -21.1        |\n",
      "|    reward             | -0.054620348 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.428        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 526        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 64.7       |\n",
      "|    reward             | -2.2508821 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 533        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -75.9      |\n",
      "|    reward             | -2.6983283 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 5.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 539        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | 64.8       |\n",
      "|    reward             | 0.47134164 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 545        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 298        |\n",
      "|    reward             | -21.651236 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 120        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 551       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -0.00548  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -16.4     |\n",
      "|    reward             | 1.2234706 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 557       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -62.1     |\n",
      "|    reward             | 1.1084722 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 563       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 121       |\n",
      "|    reward             | 1.2058527 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 570        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 53.5       |\n",
      "|    reward             | -0.4956977 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.79       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 576      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.9    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 77.4     |\n",
      "|    reward             | 0.761387 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 3.76     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 582       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 87.7      |\n",
      "|    reward             | 2.9212267 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 588       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | -2.5e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -154      |\n",
      "|    reward             | 1.1558813 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 17.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 594        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 135        |\n",
      "|    reward             | -0.5714957 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 600       |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | 4.18      |\n",
      "|    reward             | 1.9556915 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 9600        |\n",
      "|    time_elapsed       | 606         |\n",
      "|    total_timesteps    | 48000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9599        |\n",
      "|    policy_loss        | -128        |\n",
      "|    reward             | -0.85678196 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 11.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 612       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | -21.5     |\n",
      "|    reward             | 1.7918222 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.59      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 618       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 67.2      |\n",
      "|    reward             | 9.648116  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 72.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 624         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | 0.024842659 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.489       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 79         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 630        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 73.6       |\n",
      "|    reward             | 0.35147542 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.23       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zjCWfgsg3sVa",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:22:25.143384Z",
     "start_time": "2024-08-27T05:22:25.090720Z"
    }
   },
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRiOtrywfAo1"
   },
   "source": [
    "### Agent 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M2YadjfnLwgt",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:22:25.238410Z",
     "start_time": "2024-08-27T05:22:25.181376Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tCDa78rqfO_a",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:38:50.676142Z",
     "start_time": "2024-08-27T05:22:25.272376Z"
    }
   },
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4612167.11\n",
      "total_reward: 3612167.11\n",
      "total_cost: 4802.20\n",
      "total_trades: 44643\n",
      "Sharpe: 0.766\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 52       |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 44.4     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11471    |\n",
      "|    reward          | 4.617689 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 51       |\n",
      "|    time_elapsed    | 449      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 22.7     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 4.617689 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5458300.14\n",
      "total_reward: 4458300.14\n",
      "total_cost: 999.00\n",
      "total_trades: 43380\n",
      "Sharpe: 0.873\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 51       |\n",
      "|    time_elapsed    | 679      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.22     |\n",
      "|    critic_loss     | 5.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 4.617689 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 50       |\n",
      "|    time_elapsed    | 914      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.65    |\n",
      "|    critic_loss     | 5.04     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 4.617689 |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ne6M2R-WvrUQ",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:38:50.772178Z",
     "start_time": "2024-08-27T05:38:50.710131Z"
    }
   },
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gDkU-j-fCmZ"
   },
   "source": [
    "### Agent 3: PPO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y5D5PFUhMzSV",
    "ExecuteTime": {
     "end_time": "2024-08-27T05:38:50.913013Z",
     "start_time": "2024-08-27T05:38:50.881014Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gt8eIQKYM4G3",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:19:56.762019Z",
     "start_time": "2024-08-27T05:38:50.947015Z"
    }
   },
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 92         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 22         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.97556853 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119982995 |\n",
      "|    clip_fraction        | 0.183        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | 0.0131       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.44         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    reward               | 0.8722145    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 10.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 88          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016384963 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00238     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    reward               | -1.1960167  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 78.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017611418 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 43.5        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | 3.7169585   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 59.6        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3425060.25\n",
      "total_reward: 2425060.25\n",
      "total_cost: 322173.46\n",
      "total_trades: 80586\n",
      "Sharpe: 0.738\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020530546 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -9.93e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.36        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    reward               | 2.5302043   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 16.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01988323 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.4      |\n",
      "|    explained_variance   | 0.0123     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 22.9       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    reward               | 2.4574437  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 58.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 176         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020806741 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0166     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 32.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 1.709941    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 75.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01858101 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.5      |\n",
      "|    explained_variance   | -0.00432   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 17.7       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    reward               | 0.743253   |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 33.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 223         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017140314 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.0299      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.2        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | 0.7706954   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 64.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 245         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022822596 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27.2        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    reward               | 0.78147745  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 85.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 267         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019025598 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00714     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.1        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | 1.1710073   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019195363 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.01        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -0.17950325 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 85         |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 311        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01902765 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.6      |\n",
      "|    explained_variance   | 0.0103     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 78         |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    reward               | 0.39817676 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 164        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 333         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025323272 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.0116      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00875    |\n",
      "|    reward               | 0.6107603   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 355         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021487914 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.0148      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 4.94078     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031296626 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.000417    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | 0.04060862  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 406         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021008763 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.00436     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.1        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.43708792  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 68.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 431         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02600111  |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.00431    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.8        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | -0.16176578 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 77.5        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4313926.55\n",
      "total_reward: 3313926.55\n",
      "total_cost: 304535.31\n",
      "total_trades: 78757\n",
      "Sharpe: 0.834\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0285661   |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.00951    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.6         |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    reward               | -0.50080967 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 481         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029313961 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0138      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 54.1        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | -0.55257267 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 79.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 506        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02229157 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.00241    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 51.5       |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    reward               | -6.427408  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 86         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 530         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023557033 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0243      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00897    |\n",
      "|    reward               | 2.6101758   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 30.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 555         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021319075 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.00784     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51.7        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    reward               | 0.39558333  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 84          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 580         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027878689 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00939    |\n",
      "|    reward               | 6.7019653   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 604         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030455891 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -0.00379    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    reward               | -1.4484165  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 627         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034309156 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.29        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    reward               | 1.4272879   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 17.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 651         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030956734 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0128      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | 0.12257209  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 78.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 675         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025049115 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0282      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 58          |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | -1.5424211  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 703         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029447094 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | -0.00766    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    reward               | 3.0293634   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 25.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 30         |\n",
      "|    time_elapsed         | 730        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01712904 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.5      |\n",
      "|    explained_variance   | 0.012      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 29.7       |\n",
      "|    n_updates            | 290        |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    reward               | 1.1887208  |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 98         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 754         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023384677 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | -0.0102     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.9        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    reward               | 3.3574443   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 73.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026882987 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | -0.023      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.1        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | -0.7336172  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 43.6        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4735613.49\n",
      "total_reward: 3735613.49\n",
      "total_cost: 291803.13\n",
      "total_trades: 78024\n",
      "Sharpe: 0.875\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 809         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033059068 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00931     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 77.8        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.087624244 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 833         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039434798 |\n",
      "|    clip_fraction        | 0.352       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.044       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 51.7        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.57788545  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 75.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 857         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029826216 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 58.1        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00978    |\n",
      "|    reward               | 0.78443885  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 881        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03685259 |\n",
      "|    clip_fraction        | 0.343      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.02       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.2       |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    reward               | -4.6205473 |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 23.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029624326 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0129      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 54.5        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | -1.1126742  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 929         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022358436 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0248      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    reward               | -15.981083  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 140         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 952        |\n",
      "|    total_timesteps      | 79872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0315192  |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43        |\n",
      "|    explained_variance   | -0.000645  |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 23.9       |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.00745   |\n",
      "|    reward               | -2.9447048 |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 61.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 979         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028445104 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0222      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.6        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | -0.7625556  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1003        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029528268 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.7        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00926    |\n",
      "|    reward               | 0.5583214   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 142         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1025        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027456427 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.00946     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 95.6        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    reward               | 0.69668126  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 1048       |\n",
      "|    total_timesteps      | 88064      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02590228 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.3      |\n",
      "|    explained_variance   | 0.0905     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 11         |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.00587   |\n",
      "|    reward               | 0.86903554 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 19.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 1070        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028727202 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0225      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 66.9        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | 0.4277901   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 121         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1094        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020947702 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 77.4        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    reward               | 1.7041184   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 172         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 1121       |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02677683 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.4      |\n",
      "|    explained_variance   | 0.00226    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 21.5       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00465   |\n",
      "|    reward               | -4.970903  |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 43.1       |\n",
      "----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6042967.51\n",
      "total_reward: 5042967.51\n",
      "total_cost: 269506.84\n",
      "total_trades: 75538\n",
      "Sharpe: 0.962\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 84         |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 1144       |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01512213 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.4      |\n",
      "|    explained_variance   | 0.0361     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 38.3       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    reward               | 1.9180866  |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 134        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1168        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026386427 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0578      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.8        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00948    |\n",
      "|    reward               | 11.864187   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 1192        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031823076 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.00163     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 26.5        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    reward               | -0.573899   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 1217        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017894853 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | -0.2590668  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 26.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1241        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02216364  |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.0962      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 21.3        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00857    |\n",
      "|    reward               | -0.18576896 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 115         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 84        |\n",
      "|    iterations           | 52        |\n",
      "|    time_elapsed         | 1265      |\n",
      "|    total_timesteps      | 106496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0214127 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -43.7     |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 50.5      |\n",
      "|    n_updates            | 510       |\n",
      "|    policy_gradient_loss | -0.0137   |\n",
      "|    reward               | -3.412974 |\n",
      "|    std                  | 1.09      |\n",
      "|    value_loss           | 136       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1293        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028900178 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0622      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00441    |\n",
      "|    reward               | -1.4173652  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 1318       |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03905217 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.8      |\n",
      "|    explained_variance   | 0.0575     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 72.1       |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    reward               | 0.80573374 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 82.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1342        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027844824 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | 3.3188498   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1367        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028665027 |\n",
      "|    clip_fraction        | 0.306       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.3        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.000743   |\n",
      "|    reward               | 1.6920816   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1394        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029071277 |\n",
      "|    clip_fraction        | 0.328       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0624      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.4        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    reward               | 0.08419146  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 77.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1421        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041303847 |\n",
      "|    clip_fraction        | 0.335       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 64.1        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    reward               | 0.9193773   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 1446        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030592985 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0755      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34          |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    reward               | -3.1899993  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 98.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 1473        |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013912704 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0814      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14          |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | 0.09299061  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 29.9        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4712123.74\n",
      "total_reward: 3712123.74\n",
      "total_cost: 294102.78\n",
      "total_trades: 77381\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 1499        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023447365 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0076     |\n",
      "|    reward               | 0.95163393  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 81          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 1526        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029400215 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | -0.039      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 38.5        |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00969    |\n",
      "|    reward               | 1.0090063   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 84.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1552        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026307538 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | -0.00154    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.5        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.000385   |\n",
      "|    reward               | 4.6606927   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 24.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 1578        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036308452 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.1        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0095     |\n",
      "|    reward               | -0.88237405 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 56.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 83         |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 1602       |\n",
      "|    total_timesteps      | 133120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03304831 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.2      |\n",
      "|    explained_variance   | 0.0949     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 85.4       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.00181   |\n",
      "|    reward               | -1.118221  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 111        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 1627        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028649803 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.00954     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.5        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    reward               | 3.754912    |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 75.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 1651        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029471647 |\n",
      "|    clip_fraction        | 0.331       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    reward               | -1.8769306  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 22.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 1677        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023446215 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 99.2        |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    reward               | -0.16815792 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 174         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 1702        |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027948316 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0775      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 56.5        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    reward               | -4.2305655  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 1726        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024937011 |\n",
      "|    clip_fraction        | 0.315       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.7        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.000976   |\n",
      "|    reward               | -0.28974915 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 33.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 1752        |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015109512 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.0875      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 96.5        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    reward               | -0.4484831  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 117         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 1777        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015950825 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.275       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 73          |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00982    |\n",
      "|    reward               | -31.121891  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 1802       |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03975493 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.6      |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.8       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0075    |\n",
      "|    reward               | -1.4259987 |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 60.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 1827        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027346222 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.7        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    reward               | -1.921959   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 58.7        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5034952.45\n",
      "total_reward: 4034952.45\n",
      "total_cost: 250220.03\n",
      "total_trades: 74218\n",
      "Sharpe: 0.869\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 1853        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035060752 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 67.3        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    reward               | 1.0342127   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 1877        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022180844 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.0377      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.9        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    reward               | -0.7391936  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 120         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 1900        |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037776686 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.07        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | 0.00353     |\n",
      "|    reward               | -0.7923045  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 1924        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029529467 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    reward               | 2.5516791   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 145         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 1949        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020186022 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 55.1        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    reward               | -0.85226196 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 158         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 1975        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020228302 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0803      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.0073     |\n",
      "|    reward               | 1.2568928   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 83.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 2000        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02255384  |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 113         |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    reward               | -0.78085107 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 2026        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034947842 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 88.5        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    reward               | -0.05354389 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 82         |\n",
      "|    iterations           | 83         |\n",
      "|    time_elapsed         | 2055       |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02719858 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 81.3       |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    reward               | 0.50388724 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 135        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 2083        |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054328945 |\n",
      "|    clip_fraction        | 0.339       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.66        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    reward               | -0.08932081 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 2116        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036334187 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.84        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | -0.07001109 |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 2142        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036618955 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00676    |\n",
      "|    reward               | 0.38128325  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 170         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 2169        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045515522 |\n",
      "|    clip_fraction        | 0.397       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.273       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    reward               | 1.9570254   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 2196        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029661587 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.5        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    reward               | -2.451633   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 60.2        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4926194.16\n",
      "total_reward: 3926194.16\n",
      "total_cost: 272481.81\n",
      "total_trades: 75529\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 2227        |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035229202 |\n",
      "|    clip_fraction        | 0.38        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0503      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.7        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | 0.00395     |\n",
      "|    reward               | -0.1823413  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 112         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 81         |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 2253       |\n",
      "|    total_timesteps      | 184320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04538651 |\n",
      "|    clip_fraction        | 0.379      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.2      |\n",
      "|    explained_variance   | -0.0181    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 67.9       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | 0.00446    |\n",
      "|    reward               | -1.0791336 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 84.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 2279        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041038968 |\n",
      "|    clip_fraction        | 0.371       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.63        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    reward               | -1.6723089  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 19.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 2306        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045946438 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0612      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 38.8        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    reward               | -1.302448   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 87.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 2331        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047246054 |\n",
      "|    clip_fraction        | 0.387       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.0461      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.5        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | 0.00143     |\n",
      "|    reward               | 1.8191029   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 2358        |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025556825 |\n",
      "|    clip_fraction        | 0.295       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.98        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00294    |\n",
      "|    reward               | 0.41586545  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 2384        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040515326 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.066       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.000566   |\n",
      "|    reward               | -0.785951   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 84.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 2410        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031120604 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.2        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    reward               | 3.4009693   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 68.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 2437        |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044985857 |\n",
      "|    clip_fraction        | 0.383       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.6       |\n",
      "|    explained_variance   | 0.0497      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | 0.0131      |\n",
      "|    reward               | 2.0913718   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 2463        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024542056 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.7       |\n",
      "|    explained_variance   | -0.0201     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.4        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    reward               | 0.1443103   |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 59.9        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C6AidlWyvwzm",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:19:56.935130Z",
     "start_time": "2024-08-27T06:19:56.873357Z"
    }
   },
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Zpv4S0-fDBv"
   },
   "source": [
    "### Agent 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JSAHhV4Xc-bh",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:19:57.058635Z",
     "start_time": "2024-08-27T06:19:56.948133Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cuda device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OSRxNYAxdKpU",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:38:12.682353Z",
     "start_time": "2024-08-27T06:19:57.200614Z"
    }
   },
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6525903.99\n",
      "total_reward: 5525903.99\n",
      "total_cost: 1003.25\n",
      "total_trades: 54094\n",
      "Sharpe: 1.015\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 247       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -33.5     |\n",
      "|    critic_loss     | 185       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 7.3763247 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 492       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.65     |\n",
      "|    critic_loss     | 48.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 7.3763247 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 741       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.41     |\n",
      "|    critic_loss     | 25.4      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 7.3763247 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5263438.76\n",
      "total_reward: 4263438.76\n",
      "total_cost: 999.00\n",
      "total_trades: 54899\n",
      "Sharpe: 0.940\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 46        |\n",
      "|    time_elapsed    | 1003      |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -0.764    |\n",
      "|    critic_loss     | 24.6      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 7.3763247 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OkJV6V_mv2hw",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:38:12.902301Z",
     "start_time": "2024-08-27T06:38:12.781927Z"
    }
   },
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dr49PotrfG01"
   },
   "source": [
    "### Agent 5: SAC"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xwOhVjqRkCdM",
    "ExecuteTime": {
     "end_time": "2024-08-27T06:38:13.057845Z",
     "start_time": "2024-08-27T06:38:12.936299Z"
    }
   },
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cuda device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K8RSdKCckJyH",
    "ExecuteTime": {
     "end_time": "2024-08-27T07:14:40.126397Z",
     "start_time": "2024-08-27T06:38:13.170003Z"
    }
   },
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 360       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.09e+03  |\n",
      "|    critic_loss     | 600       |\n",
      "|    ent_coef        | 0.16      |\n",
      "|    ent_coef_loss   | -83.3     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 7.3387547 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5249672.24\n",
      "total_reward: 4249672.24\n",
      "total_cost: 94377.53\n",
      "total_trades: 65145\n",
      "Sharpe: 0.909\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 733      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 498      |\n",
      "|    critic_loss     | 111      |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | -91.7    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 9.378625 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 1095     |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 236      |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -64      |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 9.119692 |\n",
      "---------------------------------\n",
      "day: 2892, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6296694.25\n",
      "total_reward: 5296694.25\n",
      "total_cost: 2409.47\n",
      "total_trades: 43452\n",
      "Sharpe: 0.983\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 1469     |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 127      |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.00609  |\n",
      "|    ent_coef_loss   | -21.8    |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 9.212039 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 1832     |\n",
      "|    total_timesteps | 57860    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.8     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.00382  |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 57759    |\n",
      "|    reward          | 9.640628 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 31       |\n",
      "|    time_elapsed    | 2170     |\n",
      "|    total_timesteps | 69432    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 39.3     |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.00347  |\n",
      "|    ent_coef_loss   | 3.69     |\n",
      "|    learning_rate   | 0.0001   |\n",
      "|    n_updates       | 69331    |\n",
      "|    reward          | 9.95521  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_SpZoQgPv7GO",
    "ExecuteTime": {
     "end_time": "2024-08-27T07:14:40.370972Z",
     "start_time": "2024-08-27T07:14:40.190529Z"
    }
   },
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgGm3dQZfRks"
   },
   "source": [
    "## Save the trained agent\n",
    "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
    "\n",
    "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
    "\n",
    "For users running on your local environment, the zip files should be at \"./trained_models\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv",
    "Dr49PotrfG01"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
